{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-22T17:40:28.686842Z",
     "iopub.status.busy": "2023-11-22T17:40:28.686230Z",
     "iopub.status.idle": "2023-11-22T17:40:29.099262Z",
     "shell.execute_reply": "2023-11-22T17:40:29.098273Z",
     "shell.execute_reply.started": "2023-11-22T17:40:28.686803Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:44:08.648916Z",
     "iopub.status.busy": "2023-11-22T07:44:08.648437Z",
     "iopub.status.idle": "2023-11-22T07:44:22.044068Z",
     "shell.execute_reply": "2023-11-22T07:44:22.042934Z",
     "shell.execute_reply.started": "2023-11-22T07:44:08.648886Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install jupyter-lsp==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:44:22.047272Z",
     "iopub.status.busy": "2023-11-22T07:44:22.046462Z",
     "iopub.status.idle": "2023-11-22T07:44:40.064414Z",
     "shell.execute_reply": "2023-11-22T07:44:40.063118Z",
     "shell.execute_reply.started": "2023-11-22T07:44:22.047234Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install comet_ml transformers tensorflow-text sacrebleu bert_score rouge_score datasets sentencepiece deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:45:15.919926Z",
     "iopub.status.busy": "2023-11-22T07:45:15.919546Z",
     "iopub.status.idle": "2023-11-22T07:45:30.068881Z",
     "shell.execute_reply": "2023-11-22T07:45:30.068046Z",
     "shell.execute_reply.started": "2023-11-22T07:45:15.919894Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from comet_ml import Experiment\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # Needed for loading universal-sentence-encoder-cmlm/multilingual-preprocess\n",
    "import numpy as np\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:45:50.247596Z",
     "iopub.status.busy": "2023-11-22T07:45:50.247214Z",
     "iopub.status.idle": "2023-11-22T07:45:50.275257Z",
     "shell.execute_reply": "2023-11-22T07:45:50.274354Z",
     "shell.execute_reply.started": "2023-11-22T07:45:50.247564Z"
    }
   },
   "outputs": [],
   "source": [
    "# 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\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:45:56.036976Z",
     "iopub.status.busy": "2023-11-22T07:45:56.036164Z",
     "iopub.status.idle": "2023-11-22T07:45:56.043026Z",
     "shell.execute_reply": "2023-11-22T07:45:56.041860Z",
     "shell.execute_reply.started": "2023-11-22T07:45:56.036936Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset(input_path):\n",
    "  df = pd.read_parquet(input_path)\n",
    "  return df[[\"#1 String\", \"#2 String\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T21:09:16.895271Z",
     "iopub.status.busy": "2023-11-19T21:09:16.894319Z",
     "iopub.status.idle": "2023-11-19T21:09:24.186910Z",
     "shell.execute_reply": "2023-11-19T21:09:24.186016Z",
     "shell.execute_reply.started": "2023-11-19T21:09:16.895230Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# Download configuration from huggingface.co and cache.\n",
    "config = AutoConfig.from_pretrained(\"ThanhJamieAI/ParapharseV8_8E_4B\")\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:51:36.505480Z",
     "iopub.status.busy": "2023-11-22T07:51:36.504642Z",
     "iopub.status.idle": "2023-11-22T07:51:36.513013Z",
     "shell.execute_reply": "2023-11-22T07:51:36.512034Z",
     "shell.execute_reply.started": "2023-11-22T07:51:36.505445Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(input_path: str, sep: str, select_columns: list, rename_columns: list):\n",
    "    if input_path.split(\".\")[1] == \"txt\":\n",
    "        df = pd.read_csv(input_path, sep=sep, quoting=csv.QUOTE_NONE)\n",
    "    elif input_path.split(\".\")[1] == \"parquet\":\n",
    "        df = pd.read_parquet(input_path)\n",
    "    else:\n",
    "        df = pd.read_csv(input_path, sep=sep)\n",
    "\n",
    "    if len(select_columns) == 0:\n",
    "        return df\n",
    "    elif len(rename_columns) == 0:\n",
    "        return df[select_columns]\n",
    "    elif len(select_columns) == len(rename_columns):\n",
    "        for i in range(0, len(select_columns)):\n",
    "            df.rename(columns={select_columns[i]: rename_columns[i]}, inplace=True)\n",
    "        return df[rename_columns]\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T04:50:23.367237Z",
     "iopub.status.busy": "2023-11-16T04:50:23.366865Z",
     "iopub.status.idle": "2023-11-16T04:50:24.377028Z",
     "shell.execute_reply": "2023-11-16T04:50:24.375988Z",
     "shell.execute_reply.started": "2023-11-16T04:50:23.367205Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_parquet(\"/kaggle/input/1111111/train_quality_dataset11111.parquet\")\n",
    "# data_test['#1 String'] = data_test['#1 String'].astype('str')\n",
    "# data_test['#2 String'] = data_test['#2 String'].astype('str')\n",
    "a = data_test.iloc[0:35000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T04:50:26.279963Z",
     "iopub.status.busy": "2023-11-16T04:50:26.279601Z",
     "iopub.status.idle": "2023-11-16T04:50:26.360111Z",
     "shell.execute_reply": "2023-11-16T04:50:26.359121Z",
     "shell.execute_reply.started": "2023-11-16T04:50:26.279932Z"
    }
   },
   "outputs": [],
   "source": [
    "a.to_parquet('/kaggle/working/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:51:41.155564Z",
     "iopub.status.busy": "2023-11-22T07:51:41.155184Z",
     "iopub.status.idle": "2023-11-22T07:51:41.162827Z",
     "shell.execute_reply": "2023-11-22T07:51:41.161806Z",
     "shell.execute_reply.started": "2023-11-22T07:51:41.155534Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_prepare_dataset(raw_dataframe, tokenizer, model):\n",
    "  data_x = tokenizer(raw_dataframe['#1 String'].tolist(), padding='max_length',\n",
    "                     max_length=raw_dataframe['#1 String'].map(lambda x: len(x)).max(), return_tensors='pt')\n",
    "\n",
    "  data_y = None\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "      data_y = tokenizer(raw_dataframe['#2 String'].tolist(), padding='max_length',\n",
    "                              max_length=raw_dataframe['#2 String'].map(lambda x: len(x)).max(), return_tensors='np')\n",
    "  data_y = data_y['input_ids']\n",
    "  data_y[data_y == model.config.pad_token_id] = -100\n",
    "\n",
    "  data_x['labels'] = torch.tensor(data_y)\n",
    "\n",
    "#   data_x['decoder_input_ids'] = shift_tokens_right(data_x['labels'], model.config.pad_token_id, model.config.decoder_start_token_id)\n",
    "\n",
    "  return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:51:42.717003Z",
     "iopub.status.busy": "2023-11-22T07:51:42.716284Z",
     "iopub.status.idle": "2023-11-22T07:51:42.722960Z",
     "shell.execute_reply": "2023-11-22T07:51:42.721989Z",
     "shell.execute_reply.started": "2023-11-22T07:51:42.716970Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_prepare_test_dataset(raw_dataframe, tokenizer):\n",
    "  data_x = tokenizer(raw_dataframe['#1 String'].tolist(), padding='max_length',\n",
    "                     max_length=raw_dataframe['#1 String'].map(lambda x: len(x)).max(), return_tensors='pt')\n",
    "\n",
    "  data_y = None\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "      data_y = tokenizer(raw_dataframe['#2 String'].tolist(), add_special_tokens=False)\n",
    "\n",
    "  return data_x, data_y['input_ids'], raw_dataframe['#2 String'].tolist(), raw_dataframe['#1 String'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:51:45.451016Z",
     "iopub.status.busy": "2023-11-22T07:51:45.450269Z",
     "iopub.status.idle": "2023-11-22T07:51:45.457757Z",
     "shell.execute_reply": "2023-11-22T07:51:45.456748Z",
     "shell.execute_reply.started": "2023-11-22T07:51:45.450981Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetFromDictData(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, transform=None, target_transform=None):\n",
    "    self.data = data\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  def __len__(self):\n",
    "    if 'input_ids' in self.data:\n",
    "      return len(self.data['input_ids'])\n",
    "    else:\n",
    "      return len(self.data[list(self.data.keys())[0]])\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {key: self.data[key][idx] for key in self.data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T20:42:23.995383Z",
     "iopub.status.busy": "2023-11-19T20:42:23.994998Z",
     "iopub.status.idle": "2023-11-19T20:42:24.012558Z",
     "shell.execute_reply": "2023-11-19T20:42:24.011523Z",
     "shell.execute_reply.started": "2023-11-19T20:42:23.995350Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, experiment, hyperparameters, tokenized_labels, do_logging=True, validation_dataloader=None):\n",
    "\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters['lr_rate'])# lr=0.0005)\n",
    "\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  num_training_steps = hyperparameters['num_epochs'] * len(dataloader)\n",
    "\n",
    "  #lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.05*num_training_steps), num_training_steps=num_training_steps)\n",
    "  lr_scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=int(hyperparameters['num_warmup_steps_percent_of_all']*num_training_steps))\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  global_step = 0\n",
    "\n",
    "  experiment.log_parameters(hyperparameters)\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(hyperparameters['num_epochs']):\n",
    "    running_avg = 0.0\n",
    "    running_avg_loss = 0.0\n",
    "    epoch_step = 0\n",
    "\n",
    "    if epoch == 0 and validation_dataloader is not None:\n",
    "      # do a validation run before training\n",
    "      rouge_score, sacrebleu_score, bertscore_score, labse_score, _ = evaluate(model, validation_dataloader, device, None, untokenized_labels)\n",
    "\n",
    "      to_log = get_metrics_dict(rouge_score, sacrebleu_score, bertscore_score)\n",
    "      to_log['labse_score'] = labse_score\n",
    "\n",
    "      experiment.log_metrics(to_log, epoch=epoch)\n",
    "\n",
    "    for batch in dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      progress_bar.update(1)\n",
    "\n",
    "      if not do_logging:\n",
    "        del batch\n",
    "        continue\n",
    "\n",
    "      preds = torch.argmax(outputs.logits.detach(), dim=-1)\n",
    "      labels_np = batch['labels'].detach()\n",
    "\n",
    "      num_correct = (labels_np[labels_np != -100] == preds[labels_np != -100]).sum().item()\n",
    "      num_total = labels_np[labels_np != -100].numel()\n",
    "\n",
    "      del batch\n",
    "      del preds\n",
    "      del labels_np\n",
    "\n",
    "      #num_correct = (preds == batch['labels'] and batch['labels'] != -100).sum().item()\n",
    "      #num_total = batch['labels'][batch['labels'] != -100].numel()\n",
    "\n",
    "      batch_acc = (float(num_correct)/num_total)\n",
    "\n",
    "      epoch_step += 1\n",
    "      global_step += 1\n",
    "\n",
    "      the_loss = loss.item()\n",
    "\n",
    "      running_avg = ((epoch_step-1)/float(epoch_step))*running_avg + batch_acc/float(epoch_step)\n",
    "      running_avg_loss = ((epoch_step-1)/float(epoch_step))*running_avg_loss + the_loss/float(epoch_step)\n",
    "\n",
    "      progress_bar.set_postfix_str(f\"batch_loss={the_loss:.5f} epoch_running_avg_loss={running_avg_loss:.5f} batch_acc={batch_acc:.5f}\\tepoch_running_avg_acc={running_avg:.5f}\", refresh=True)\n",
    "\n",
    "      experiment.log_metric('batch_acc', batch_acc, step=global_step)\n",
    "      experiment.log_metric(f'batch_acc_epoch_{epoch}', batch_acc, step=epoch_step)\n",
    "\n",
    "      experiment.log_metric('batch_loss', the_loss, step=global_step)\n",
    "\n",
    "      experiment.log_metric('per_epoch_running_avg_acc', running_avg, step=global_step)\n",
    "      experiment.log_metric('per_epoch_running_avg_loss', running_avg_loss, step=global_step)\n",
    "\n",
    "    if validation_dataloader is not None:\n",
    "      rouge_score, sacrebleu_score, bertscore_score, labse_score, _ = evaluate(model, validation_dataloader, device, None, untokenized_labels)\n",
    "\n",
    "      to_log = get_metrics_dict(rouge_score, sacrebleu_score, bertscore_score)\n",
    "      to_log['labse_score'] = labse_score\n",
    "\n",
    "      experiment.log_metrics(to_log, epoch=epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:51:50.463635Z",
     "iopub.status.busy": "2023-11-22T07:51:50.463244Z",
     "iopub.status.idle": "2023-11-22T07:51:50.482953Z",
     "shell.execute_reply": "2023-11-22T07:51:50.482154Z",
     "shell.execute_reply.started": "2023-11-22T07:51:50.463604Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_grad_acc(model, dataloader, experiment, hyperparameters, tokenized_labels, do_logging=True, validation_dataloader=None):\n",
    "\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparameters['lr_rate'])# lr=0.0005)\n",
    "\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  num_training_steps = hyperparameters['num_epochs'] * len(dataloader)\n",
    "  effective_num_training_steps = hyperparameters['num_epochs'] * (len(dataloader)/hyperparameters['num_grad_acc_steps'])\n",
    "\n",
    "  #lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.05*num_training_steps), num_training_steps=effective_num_training_steps)\n",
    "  lr_scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=int(hyperparameters['num_warmup_steps_percent_of_all']*effective_num_training_steps))\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  global_step = 0\n",
    "\n",
    "  experiment.log_parameters(hyperparameters)\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(hyperparameters['num_epochs']):\n",
    "    running_avg = 0.0\n",
    "    running_avg_loss = 0.0\n",
    "    epoch_step = 0\n",
    "\n",
    "    if epoch == 0 and validation_dataloader is not None:\n",
    "      # do a validation run before training\n",
    "      rouge_score, sacrebleu_score, bertscore_score, labse_score, _ = evaluate(model, validation_dataloader, device, None, untokenized_labels)\n",
    "\n",
    "      to_log = get_metrics_dict(rouge_score, sacrebleu_score, bertscore_score)\n",
    "      to_log['labse_score'] = labse_score\n",
    "\n",
    "      experiment.log_metrics(to_log, epoch=epoch)\n",
    "\n",
    "    for batch_num, batch in enumerate(dataloader):\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss / hyperparameters['num_grad_acc_steps']\n",
    "      loss.backward()\n",
    "\n",
    "      if ((batch_num+1) % hyperparameters['num_grad_acc_steps']) == 0 or batch_num+1 == len(dataloader):\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "      progress_bar.update(1)\n",
    "\n",
    "      if not do_logging:\n",
    "        del batch\n",
    "        continue\n",
    "\n",
    "      preds = torch.argmax(outputs.logits.detach(), dim=-1)\n",
    "      labels_np = batch['labels'].detach()\n",
    "\n",
    "      num_correct = (labels_np[labels_np != -100] == preds[labels_np != -100]).sum().item()\n",
    "      num_total = labels_np[labels_np != -100].numel()\n",
    "\n",
    "      del batch\n",
    "      del preds\n",
    "      del labels_np\n",
    "\n",
    "      #num_correct = (preds == batch['labels'] and batch['labels'] != -100).sum().item()\n",
    "      #num_total = batch['labels'][batch['labels'] != -100].numel()\n",
    "\n",
    "      batch_acc = (float(num_correct)/num_total)\n",
    "\n",
    "      epoch_step += 1\n",
    "      global_step += 1\n",
    "\n",
    "      the_loss = loss.item()\n",
    "\n",
    "      running_avg = ((epoch_step-1)/float(epoch_step))*running_avg + batch_acc/float(epoch_step)\n",
    "      running_avg_loss = ((epoch_step-1)/float(epoch_step))*running_avg_loss + the_loss/float(epoch_step)\n",
    "\n",
    "      progress_bar.set_postfix_str(f\"batch_loss={the_loss:.5f} epoch_running_avg_loss={running_avg_loss:.5f} batch_acc={batch_acc:.5f}\\tepoch_running_avg_acc={running_avg:.5f}\", refresh=True)\n",
    "\n",
    "      experiment.log_metric('batch_acc', batch_acc, step=global_step)\n",
    "      experiment.log_metric(f'batch_acc_epoch_{epoch}', batch_acc, step=epoch_step)\n",
    "\n",
    "      experiment.log_metric('batch_loss', the_loss, step=global_step)\n",
    "\n",
    "      experiment.log_metric('per_epoch_running_avg_acc', running_avg, step=global_step)\n",
    "      experiment.log_metric('per_epoch_running_avg_loss', running_avg_loss, step=global_step)\n",
    "\n",
    "    if validation_dataloader is not None:\n",
    "      rouge_score, sacrebleu_score, bertscore_score, labse_score, _ = evaluate(model, validation_dataloader, device, None, untokenized_labels)\n",
    "\n",
    "      to_log = get_metrics_dict(rouge_score, sacrebleu_score, bertscore_score)\n",
    "      to_log['labse_score'] = labse_score\n",
    "\n",
    "      experiment.log_metrics(to_log, epoch=epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:51:56.472127Z",
     "iopub.status.busy": "2023-11-22T07:51:56.471763Z",
     "iopub.status.idle": "2023-11-22T07:51:56.480574Z",
     "shell.execute_reply": "2023-11-22T07:51:56.479544Z",
     "shell.execute_reply.started": "2023-11-22T07:51:56.472100Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_labSE_score(outputs, labels):\n",
    "\n",
    "  def normalization(embeds):\n",
    "    norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)\n",
    "    return embeds/norms\n",
    "\n",
    "  preprocessor = hub.KerasLayer(\n",
    "      \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "  encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\")\n",
    "\n",
    "  dataset = DatasetFromDictData({'outputs': outputs, 'labels': labels})\n",
    "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "  similarities = []\n",
    "\n",
    "  for batch in dataloader:\n",
    "    predictions = tf.constant(batch['outputs'])\n",
    "    references = tf.constant(batch['labels'])\n",
    "\n",
    "    predictions_embeds = encoder(preprocessor(predictions))[\"default\"]\n",
    "    references_embeds = encoder(preprocessor(references))[\"default\"]\n",
    "\n",
    "    # For semantic similarity tasks, apply l2 normalization to embeddings\n",
    "    predictions_embeds = normalization(predictions_embeds)\n",
    "    references_embeds = normalization(references_embeds)\n",
    "\n",
    "    similarities.extend(np.matmul(predictions_embeds, np.transpose(references_embeds)).diagonal().tolist())\n",
    "\n",
    "  return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:52:16.596530Z",
     "iopub.status.busy": "2023-11-22T07:52:16.596153Z",
     "iopub.status.idle": "2023-11-22T07:52:16.604105Z",
     "shell.execute_reply": "2023-11-22T07:52:16.603050Z",
     "shell.execute_reply.started": "2023-11-22T07:52:16.596497Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds_and_labels_for_eval_test(model, evaluation_dataloader, device):\n",
    "  decoded_outputs = []\n",
    "  tokenized_outputs = []\n",
    "\n",
    "  eval_progress_bar = tqdm(range(len(evaluation_dataloader)))\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in evaluation_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "      curr_outp = model.generate(**batch, max_length=300, num_beams=3, early_stopping=True)\n",
    "\n",
    "      del batch\n",
    "\n",
    "      decoded_output = tokenizer.batch_decode(curr_outp.tolist(), skip_special_tokens=True)\n",
    "\n",
    "      # dirty trick to get rid of the special tokens and have the raw tokens instead of decoded output\n",
    "      tokenized_output = tokenizer(decoded_output, add_special_tokens=False)['input_ids']\n",
    "\n",
    "      decoded_outputs.extend(decoded_output)\n",
    "      tokenized_outputs.extend(tokenized_output)\n",
    "\n",
    "      eval_progress_bar.update(1)\n",
    "\n",
    "  return decoded_outputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:52:19.288013Z",
     "iopub.status.busy": "2023-11-22T07:52:19.287498Z",
     "iopub.status.idle": "2023-11-22T07:52:19.304899Z",
     "shell.execute_reply": "2023-11-22T07:52:19.303823Z",
     "shell.execute_reply.started": "2023-11-22T07:52:19.287971Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, evaluation_dataloader, device, tokenized_labels, untokenized_labels, untokenized_inputs=None):\n",
    "\n",
    "  decoded_outputs, tokenized_outputs = get_preds_and_labels_for_eval_test(model, evaluation_dataloader, device)\n",
    "  references_for_sacrebleu = None\n",
    "\n",
    "  # if there are more than 1500 samples, reduce them to 1500 by random sampling\n",
    "  # to have a reasonable run time.\n",
    "  if len(tokenized_labels) > 5000:\n",
    "    print(f'{len(tokenized_labels)} samples, reducing down to 5000')\n",
    "  do = np.array(decoded_outputs, dtype=object)\n",
    "  to = np.array(tokenized_outputs, dtype=object)\n",
    "  tl = np.array(tokenized_labels, dtype=object)\n",
    "  ul = np.array(untokenized_labels, dtype=object)\n",
    "  ui = np.array(untokenized_inputs, dtype=object)\n",
    "\n",
    "  indicies = list(range(0, len(tokenized_labels)))\n",
    "  np.random.shuffle(indicies)\n",
    "\n",
    "  indicies = indicies[0:4999]\n",
    "\n",
    "  decoded_outputs = do[indicies].tolist()\n",
    "  tokenized_outputs = to[indicies].tolist()\n",
    "  tokenized_labels = tl[indicies].tolist()\n",
    "  untokenized_labels = ul[indicies].tolist()\n",
    "  untokenized_inputs = ui[indicies].tolist()\n",
    "\n",
    "  label_references_for_sacrebleu = [[lab] for lab in untokenized_labels]\n",
    "  input_references_for_sacrebleu = [[inp] for inp in untokenized_inputs]\n",
    "\n",
    "  print(f'new len:{len(tokenized_labels)}')\n",
    "\n",
    "  rouge = load_metric('rouge')\n",
    "  sacrebleu = load_metric('sacrebleu')\n",
    "  bertscore = load_metric('bertscore')\n",
    "\n",
    "  label_output_rouge_score = rouge.compute(predictions=decoded_outputs, references=untokenized_labels, rouge_types=['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'rougeLsum'])\n",
    "  print('rouge1 computed')\n",
    "  label_output_sacrebleu_score = sacrebleu.compute(predictions=decoded_outputs, references=label_references_for_sacrebleu)\n",
    "  print('sacrebleu1 computed')\n",
    "  label_output_bertscore_score = bertscore.compute(predictions=decoded_outputs, references=untokenized_labels, lang='en')\n",
    "  print('bertscore1 computed')\n",
    "  label_output_labse_score = calculate_labSE_score(decoded_outputs, untokenized_labels)\n",
    "  print('labsescore1 computed')\n",
    "\n",
    "  if untokenized_inputs is not None:\n",
    "    print('vleze')\n",
    "    references_for_sacrebleu = [[lab] for lab in untokenized_inputs]\n",
    "\n",
    "    input_output_rouge_score = rouge.compute(predictions=decoded_outputs, references=untokenized_inputs, rouge_types=['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'rougeLsum'])\n",
    "    print('rouge2 computed')\n",
    "    input_output_sacrebleu_score = sacrebleu.compute(predictions=decoded_outputs, references=input_references_for_sacrebleu)\n",
    "    print('sacrebleu2 computed')\n",
    "    input_output_bertscore_score = bertscore.compute(predictions=decoded_outputs, references=untokenized_inputs, lang='en')\n",
    "    print('bertscore2 computed')\n",
    "    input_output_labse_score = calculate_labSE_score(decoded_outputs, untokenized_inputs)\n",
    "    print('labsescore2 computed')\n",
    "\n",
    "    return label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score, input_output_rouge_score, input_output_sacrebleu_score, input_output_bertscore_score, input_output_labse_score, decoded_outputs, untokenized_inputs, untokenized_labels\n",
    "  else:\n",
    "    print('else')\n",
    "    return label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score, decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:52:48.737190Z",
     "iopub.status.busy": "2023-11-22T07:52:48.736829Z",
     "iopub.status.idle": "2023-11-22T07:52:48.747875Z",
     "shell.execute_reply": "2023-11-22T07:52:48.746853Z",
     "shell.execute_reply.started": "2023-11-22T07:52:48.737161Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_directly(device, untokenized_labels, untokenized_inputs, untokenized_outputs):\n",
    "\n",
    "  label_references_for_sacrebleu = [[lab] for lab in untokenized_labels]\n",
    "  input_references_for_sacrebleu = [[inp] for inp in untokenized_inputs]\n",
    "\n",
    "  print(f'new len:{len(untokenized_labels)}')\n",
    "\n",
    "  rouge = load_metric('rouge')\n",
    "  sacrebleu = load_metric('sacrebleu')\n",
    "  bertscore = load_metric('bertscore')\n",
    "\n",
    "  label_output_rouge_score = rouge.compute(predictions=untokenized_outputs, references=untokenized_labels, rouge_types=['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'rougeLsum'])\n",
    "  print('rouge1 computed')\n",
    "  label_output_sacrebleu_score = sacrebleu.compute(predictions=untokenized_outputs, references=label_references_for_sacrebleu)\n",
    "  print('sacrebleu1 computed')\n",
    "  label_output_bertscore_score = bertscore.compute(predictions=untokenized_outputs, references=untokenized_labels, lang='en')\n",
    "  print('bertscore1 computed')\n",
    "  label_output_labse_score = calculate_labSE_score(untokenized_outputs, untokenized_labels)\n",
    "  print('labsescore1 computed')\n",
    "\n",
    "  if untokenized_inputs is not None:\n",
    "    print('vleze')\n",
    "    references_for_sacrebleu = [[lab] for lab in untokenized_inputs]\n",
    "\n",
    "    input_output_rouge_score = rouge.compute(predictions=untokenized_outputs, references=untokenized_inputs, rouge_types=['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'rougeLsum'])\n",
    "    print('rouge2 computed')\n",
    "    input_output_sacrebleu_score = sacrebleu.compute(predictions=untokenized_outputs, references=input_references_for_sacrebleu)\n",
    "    print('sacrebleu2 computed')\n",
    "    input_output_bertscore_score = bertscore.compute(predictions=untokenized_outputs, references=untokenized_inputs, lang='en')\n",
    "    print('bertscore2 computed')\n",
    "    input_output_labse_score = calculate_labSE_score(untokenized_outputs, untokenized_inputs)\n",
    "    print('labsescore2 computed')\n",
    "\n",
    "    return label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score, input_output_rouge_score, input_output_sacrebleu_score, input_output_bertscore_score, input_output_labse_score\n",
    "  else:\n",
    "    print('else')\n",
    "    return label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:52:51.618436Z",
     "iopub.status.busy": "2023-11-22T07:52:51.617368Z",
     "iopub.status.idle": "2023-11-22T07:52:51.626951Z",
     "shell.execute_reply": "2023-11-22T07:52:51.626078Z",
     "shell.execute_reply.started": "2023-11-22T07:52:51.618401Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics_dict(label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score,\n",
    "                     input_output_rouge_score, input_output_sacrebleu_score, input_output_bertscore_score, input_output_labse_score):\n",
    "  metrics = {}\n",
    "  for rouge_score_type, rouge_score in zip(['input_output', 'label_output'], [input_output_rouge_score, label_output_rouge_score]):\n",
    "    for suffix in ['1', '2', '3', '4', 'L', 'Lsum']:\n",
    "      metrics[f'{rouge_score_type}_rouge{suffix}_precision'] = rouge_score[f'rouge{suffix}'].mid.precision\n",
    "      metrics[f'{rouge_score_type}_rouge{suffix}_recall'] = rouge_score[f'rouge{suffix}'].mid.recall\n",
    "      metrics[f'{rouge_score_type}_rouge{suffix}_fmeasure'] = rouge_score[f'rouge{suffix}'].mid.fmeasure\n",
    "\n",
    "  metrics['input_output_sacrebleu_score'] = input_output_sacrebleu_score['score']\n",
    "  metrics['label_output_sacrebleu_score'] = label_output_sacrebleu_score['score']\n",
    "\n",
    "  metrics['input_output_bertscore_f1'] = np.mean(input_output_bertscore_score['f1'])\n",
    "  metrics['input_output_bertscore_precision'] = np.mean(input_output_bertscore_score['precision'])\n",
    "  metrics['input_output_bertscore_recall'] = np.mean(input_output_bertscore_score['recall'])\n",
    "\n",
    "  metrics['label_output_bertscore_f1'] = np.mean(label_output_bertscore_score['f1'])\n",
    "  metrics['label_output_bertscore_precision'] = np.mean(label_output_bertscore_score['precision'])\n",
    "  metrics['label_output_bertscore_recall'] = np.mean(label_output_bertscore_score['recall'])\n",
    "\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:53:20.157175Z",
     "iopub.status.busy": "2023-11-22T07:53:20.156482Z",
     "iopub.status.idle": "2023-11-22T07:53:20.163264Z",
     "shell.execute_reply": "2023-11-22T07:53:20.162207Z",
     "shell.execute_reply.started": "2023-11-22T07:53:20.157142Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    'ParapharseV13_10E_4B': {\n",
    "        'experiment_name': 'Paraphraser_Thesis',\n",
    "        'model_name': 'ThanhJamieAI/ParapharseV8_8E_4B',\n",
    "        'hyperparameters': {\n",
    "            'num_epochs': 10,\n",
    "            'batch_size': 4,\n",
    "            'optimizer': 'AdamW',\n",
    "            'dataset': 'combine-',\n",
    "            'lr_rate': 1e-4,\n",
    "            'num_warmup_steps_percent_of_all': 0.1,\n",
    "            'lr_schedule': 'get_constant_schedule_with_warmup',\n",
    "            'num_grad_acc_steps': 8\n",
    "        },\n",
    "        'read_dataset_function' : read_dataset,\n",
    "        'dataset_path_train': '/kaggle/input/final-train-parapharse/train_set/train_quality_dataset.parquet',\n",
    "        'dataset_path_valid': '/kaggle/input/final-train-parapharse/validation_set/validation_dataset.parquet',\n",
    "        'dataset_preprocessing_function': None,\n",
    "        'training_function': train_grad_acc,\n",
    "        'tokenizer_class': PegasusTokenizer,\n",
    "        'model_class': PegasusForConditionalGeneration\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T11:25:58.180936Z",
     "iopub.status.busy": "2023-11-13T11:25:58.180294Z",
     "iopub.status.idle": "2023-11-13T11:25:58.185406Z",
     "shell.execute_reply": "2023-11-13T11:25:58.184274Z",
     "shell.execute_reply.started": "2023-11-13T11:25:58.180901Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T20:51:23.435671Z",
     "iopub.status.busy": "2023-11-19T20:51:23.435286Z",
     "iopub.status.idle": "2023-11-19T20:51:23.440517Z",
     "shell.execute_reply": "2023-11-19T20:51:23.439538Z",
     "shell.execute_reply.started": "2023-11-19T20:51:23.435641Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-19T20:51:25.980028Z",
     "iopub.status.busy": "2023-11-19T20:51:25.979053Z",
     "iopub.status.idle": "2023-11-19T20:51:25.984113Z",
     "shell.execute_reply": "2023-11-19T20:51:25.983182Z",
     "shell.execute_reply.started": "2023-11-19T20:51:25.979992Z"
    }
   },
   "outputs": [],
   "source": [
    "comet.end()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T07:53:39.184539Z",
     "iopub.status.busy": "2023-11-22T07:53:39.183579Z",
     "iopub.status.idle": "2023-11-22T16:41:24.863798Z",
     "shell.execute_reply": "2023-11-22T16:41:24.862994Z",
     "shell.execute_reply.started": "2023-11-22T07:53:39.184505Z"
    }
   },
   "outputs": [],
   "source": [
    "for experiment_name, experiment_data in experiments.items():\n",
    "  comet = Experiment(\"eBzP9Wi5IQwGgIShopGb65swt\", project_name='Capstone_ProjectFPTU_Fa23')\n",
    "  comet.set_name(experiment_name)\n",
    "\n",
    "  comet.log_parameters(experiment_data['hyperparameters'])\n",
    "\n",
    "  tokenizer = experiment_data['tokenizer_class'].from_pretrained(experiment_data['model_name'])\n",
    "  model = experiment_data['model_class'].from_pretrained(experiment_data['model_name'])\n",
    "#   model = PegasusForCausalLM.from_pretrained(model_name_or_path)\n",
    "#   model = get_peft_model(model, peft_config)\n",
    "  dataset_train = experiment_data['read_dataset_function'](experiment_data['dataset_path_train'])\n",
    "  dataset_valid = experiment_data['read_dataset_function'](experiment_data['dataset_path_valid'])\n",
    "\n",
    "  if experiment_data['dataset_preprocessing_function'] is not None:\n",
    "    # preprocess the data with the given preprocessing function\n",
    "    experiment_data['dataset_preprocessing_function'](dataset_train)\n",
    "    experiment_data['dataset_preprocessing_function'](dataset_valid)\n",
    "\n",
    "  data_train = tokenize_and_prepare_dataset(dataset_train, tokenizer, model)\n",
    "  data_val, tokenized_labels, untokenized_labels, untokenized_input = tokenize_and_prepare_test_dataset(dataset_valid, tokenizer)\n",
    "\n",
    "  train_dataset = DatasetFromDictData(data_train)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=experiment_data['hyperparameters']['batch_size'], num_workers=2)\n",
    "\n",
    "  val_dataset = DatasetFromDictData(data_val)\n",
    "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=experiment_data['hyperparameters']['batch_size'], num_workers=2)\n",
    "\n",
    "  # call the corresponding training function as given in the experiment dict\n",
    "  experiment_data['training_function'](model, train_dataloader, comet, experiment_data['hyperparameters'], tokenized_labels, do_logging=True)\n",
    "  torch.save(model.state_dict(), f'/kaggle/working/{comet.get_name().replace(\"/\", \"_\")}.pt')\n",
    "\n",
    "  # free GPU RAM\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "  label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score, input_output_rouge_score, input_output_sacrebleu_score, input_output_bertscore_score, input_output_labse_score, untokenized_output, untokenized_input, untokenized_labels = evaluate(model, val_dataloader, device, tokenized_labels, untokenized_labels, untokenized_input)\n",
    "\n",
    "  # Save data for later computation of RougeWE score\n",
    "  dict_to_save = {\n",
    "      'experiment_url': comet.url,\n",
    "      'experiment_name': comet.get_name(),\n",
    "      'experiment_key': comet.get_key(),\n",
    "      'inputs_strings': untokenized_input,\n",
    "      'model_outputs_strings': untokenized_output,\n",
    "      'labels_strings': untokenized_labels\n",
    "  }\n",
    "\n",
    "  with open(f'/kaggle/working/{comet.get_name().replace(\"/\", \"_\")}.pickle', 'wb+') as handle:\n",
    "      pickle.dump(dict_to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  to_log = get_metrics_dict(label_output_rouge_score, label_output_sacrebleu_score, label_output_bertscore_score, label_output_labse_score,\n",
    "  input_output_rouge_score, input_output_sacrebleu_score, input_output_bertscore_score, input_output_labse_score)\n",
    "\n",
    "  to_log['label_output_labse_score'] = label_output_labse_score\n",
    "  to_log['input_output_labse_score'] = input_output_labse_score\n",
    "\n",
    "  comet.log_metrics(to_log, epoch=experiment_data['hyperparameters']['num_epochs'] + 1)\n",
    "\n",
    "  for key in list(to_log.keys()):\n",
    "    to_log[f'test_{key}'] = to_log.pop(key)\n",
    "\n",
    "  comet.log_metrics(to_log)\n",
    "\n",
    "  comet.log_others(to_log)\n",
    "\n",
    "  comet.log_other('test_full_input_output_bleu_score', str(input_output_sacrebleu_score))\n",
    "  comet.log_other('test_full_label_output_bleu_score', str(label_output_sacrebleu_score))\n",
    "\n",
    "  comet.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T16:45:24.296477Z",
     "iopub.status.busy": "2023-11-22T16:45:24.296052Z",
     "iopub.status.idle": "2023-11-22T16:45:24.316687Z",
     "shell.execute_reply": "2023-11-22T16:45:24.315688Z",
     "shell.execute_reply.started": "2023-11-22T16:45:24.296446Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T10:20:26.263466Z",
     "iopub.status.idle": "2023-11-13T10:20:26.263938Z",
     "shell.execute_reply": "2023-11-13T10:20:26.263726Z",
     "shell.execute_reply.started": "2023-11-13T10:20:26.263703Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer.save_pretrained(\"/kaggle/working/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T10:20:26.265354Z",
     "iopub.status.idle": "2023-11-13T10:20:26.265820Z",
     "shell.execute_reply": "2023-11-13T10:20:26.265613Z",
     "shell.execute_reply.started": "2023-11-13T10:20:26.265589Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T16:45:51.528389Z",
     "iopub.status.busy": "2023-11-22T16:45:51.527450Z",
     "iopub.status.idle": "2023-11-22T16:46:02.633734Z",
     "shell.execute_reply": "2023-11-22T16:46:02.632566Z",
     "shell.execute_reply.started": "2023-11-22T16:45:51.528353Z"
    }
   },
   "outputs": [],
   "source": [
    "!huggingface-cli repo create \"ParapharseV13_10E_4B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T10:20:26.269312Z",
     "iopub.status.idle": "2023-11-13T10:20:26.269744Z",
     "shell.execute_reply": "2023-11-13T10:20:26.269551Z",
     "shell.execute_reply.started": "2023-11-13T10:20:26.269524Z"
    }
   },
   "outputs": [],
   "source": [
    "# /kaggle/working/ParapharseV6_8E_2B.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T10:20:26.270961Z",
     "iopub.status.idle": "2023-11-13T10:20:26.271286Z",
     "shell.execute_reply": "2023-11-13T10:20:26.271141Z",
     "shell.execute_reply.started": "2023-11-13T10:20:26.271124Z"
    }
   },
   "outputs": [],
   "source": [
    "# i = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T16:46:58.161647Z",
     "iopub.status.busy": "2023-11-22T16:46:58.160814Z",
     "iopub.status.idle": "2023-11-22T16:47:47.733111Z",
     "shell.execute_reply": "2023-11-22T16:47:47.731925Z",
     "shell.execute_reply.started": "2023-11-22T16:46:58.161607Z"
    }
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(repo_id = 'ParapharseV13_10E_4B')\n",
    "tokenizer.push_to_hub(repo_id = 'ParapharseV13_10E_4B')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3966022,
     "sourceId": 6905513,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3983420,
     "sourceId": 6936636,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3993299,
     "sourceId": 6952693,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3993367,
     "sourceId": 6952793,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3995423,
     "sourceId": 6956120,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3995479,
     "sourceId": 6956202,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3996436,
     "sourceId": 6957568,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
